&emsp;&emsp;**指令模型核心考察学习用户偏好分布的范围能力。**
### 5.2.1 自动评估
**1、指标测评**    
-loss曲线：train loss是训练数据上的损失，衡量模型在训练集上的拟合能力；val loss是验证集上的损失，衡量模型在未见过的数据上的拟合能力，也可以说是泛化能力。  
-rouge：rouge是ISI在2004提出，其总体思想就是召回率，假如给定标准response为reference，大模型生成的response为candidate，真实答案response句子长度为n，candidate中有m个单词出现在reference，m/n就是Bleu的1-gram计算公式。
-bleu：bleu是IBM在2002年提出，其总体思想就是准确率，假如给定标准response为reference，大模型生成的response为candidate，大模型response句子长度为n，candidate中有m个单词出现在reference，m/n就是Bleu的1-gram计算公式。  
-others：对于给定的测试数据集，准确率accuracy指分类器正确的样本数与总样本数之比；精确率precision指预测为正的样本中，实际为正的比例；召回率recall指实际为正的样本被判断为正样本的比例。  
precision和recall是矛盾的度量，为了综合考虑这两个指标，f-score核心思想在尽可能的提高Precision和recall同时，希望两者差异尽可能小。

**2、benchmark测评**  
-HELM，斯坦福基础模型研究中心旨在从多方面分析如何整体评价当前越来越大的语言模型，从三个方面整体测评LLM：1、覆盖场景广泛：全面的评估应该提供一个自上而下的分类，并明确所有主要场景和确实的度量标准。HELM在场景和指标上采用自上而下的分类方法，共42个不同的场景，有助于系统性选择场景和指标，还可以清楚的表明当前模型在什么场景下效果不佳；2、多指标衡量。社会有益的系统反映了许多价值观，而不仅仅是准确性，全面的评估应该代表这些多元的预期，并针对考虑的每个场景评估所需的能力。3、标准化，为了评估这些差距这些很大的模型，HELM将所有的语言模型迁移为了few-shot prompting。  
-C-Eval，多领域多任务多层级的中文LLM测评基准。1、覆盖更广泛的领域：人文，社科，理工，其他专业四个大方向，52个学科（微积分，线代），从中学到大学研究生以及职业考试，一共13984道题目的中文知识和推理型测试集。2、具有四种不同的难度：问题分布从简单到困难，特别是C-Eval的Hard基准是中国第一个从提供复杂推理问题的基准。3、更少的数据泄露：基准中问题大多来自模拟考试的PDF或Microsoft Word文件，这些文件由团队成员进一步处理得到。
C-Eval测评模型强弱的核心指标：-知识-，希望模型可以通用，可以在不同领域都贡献生产力，这自然需要模型知道各个领域的知识；同时希望模型不要胡说八道，不知为不知，这也需要扩大模型知识；斯坦福的HELM英文评价榜单中，一个重要的结论是，模型的参数量可以被用来储存知识；GPT-4发布博客中，首先就是列出模型在各个学科考试上的效果，作为模型能力的衡量标准。-推理-，推理能力是在知识基础上进一步上升的能力，它代表着模型是否能做很困难，很复杂的事情。推理很重要的论点是：GPT-4发布博客中，OpenAI明确写道“The difference comes out when the complexity of the task reaches a sufficient threshold”.
-SuperCLUE，中文通用大模型综合性测评基准。SuperCLUE从三个不同的维度评价模型的能力，基础能力，包括了常见的代表性的模型能力，如对话、逻辑推理、角色扮演、代码、生成与创作等10项能力；专业能力：包括了中学、大学与专业考试，涵盖了从数学、物理、地理到社会科学等50多项能力；中文特性能力：针对有中文特点的任务，包括了中文成语、诗歌、文学、字形等10多种能力。  
-Open LLM Leaderboard，面向HF社区开放的测评，由HuggingFace建立固定的benchmarks数据集，对社区内的大模型在多个任务的数据集中进行统一测评。包括AI2 Reasoning Challenge、HellaSwag、MMLU、TruthfulQA这4个benchmark数据集。  

**3、模型裁判测评**  
-PandaLM。北大及微软亚洲研究院联合训练了一个专门用于评估大模型性能的裁判大模型，并提供了接口，仅需三行代码就可以调用PandaLM大模型进行保护隐私、可靠、可复现及廉价的大模型评估。  

### 5.2.1 人工评估
-Chatbot Arena聊天机器人匿名竞技场。2023年5月，LMSYS org一个开放的研究组织建立对市面上的主流LLM进行基准测试，建立聊天机器人竞技场，通过众包方式在大语言模型进行随机匿名的1v1 battle方式，并基于Elo评级系统得出排名。  