### 3.2.1 强化学习框架
强化学习的框架和训练框架较为不一致，但他们本质都是trainer
近端策略优化算法是OpenAI在2017年提出的一种强化学习算法，我们的目标就是找到那些可能获得更多奖励的动作，使他们对应的概率更大
### 3.2.2 框架实践
（1）DeepSpeed-chat
```python
for epoch in range(args.num_train_epoch):
    for step, (batch_prompt, batch_unsupervised) in enumerate(zip(prompt_train_dataloader, unsupervised_train_dataloader)):
        # 建立经验池数据和正向传播(actor和reward）获得logits
        out = trainer.generate_experience(batch_prompt['prompt']，batch_prompt['prompt_att_mask'])
        unsup_dataset = unsup_mini_dataset.add(batch_unsupervised)

        exp_dataset = exp_mini_dataset.add(out)

        for ppo_ep in range(args.ppo_epochs):
            for i,(exp_data, unsup_data) in enumerate(zip(exp_dataset, unsup_dataset)):

                # 计算kl reward和advatage
                with torch.no_grad():
                    old_rewards = self.compute_rewards(prompts, log_probs, ref_log_probs, reward_score, action_mask)
                    advantages, returns = self.get_advantages_and_returns(old_values, old_rewards, start)

                # 计算Loss 和反向传播(actor和critic）
                actor_loss = self.actor_loss_fn(actor_log_prob[:, start:], log_probs[:, start:], advantage, action_mask[:, start])
                self.actor_model.backward(actor_loss)
                self.actor_model.step()

                critic_loss = self.critic_loss_fn(value[:, start:], old_values[:, start:], returns, action_mask[:, start:])
                self.critic_model.backward(critic_loss)
                self.critic_model.step()
```

（2）trlx

（3）trl
Transformer Reinforcement Learning
```python
for epoch in range(args.num_train_epoch):
    for query in dataloader:
        # 正向传播(actor和reward）
        response = model.generate(query)
        reward = reward_model(response)
        # 计算kl reward和advatage
        with torch.no_grad:
            rewards = self.compute_rewards(scores, active_full_logprobs, ref_full_logprobs, masks)
            values, advantages, returns = self.compute_advatage(values, rewards, masks)

        for  _ in range(self.config.ppo_epochs):
            for mini_vatch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                # 计算Loss
                loss_p, loss_v, train_stats = self.loss(old_log_probs, values, logits, vpreds, logprobs, mask, advatages, returns)
                loss = loss_p + loss_v
                # 反向传播（actor）
                self.accelerator.backward(loss)
                self.optimizer.step()
                self.optimizer.zero_grad()
```


（4）ColossalAI-Chat



