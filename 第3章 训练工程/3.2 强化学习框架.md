### 3.2.1 强化学习框架
&emsp;&emsp;强化学习的框架和训练框架较为不一致，但他们本质都是trainer。2020年，OpenAI率先将RLHF用于文本摘要，使大模型超越人类水准，并首次提出RLHF三部曲范式，2023年GPT-4报告指出GPT-4的出色性能得益于RLHF。近端策略优化算法是OpenAI在2017年提出的一种强化学习算法，我们的目标就是找到那些可能获得更多奖励的动作，使他们对应的概率更大。    
&emsp;&emsp;2021年，Anthropic首次提出了LLM的通用目标，即和人类价值观对齐的HHH（helpful，honest，and harmless）评估标准，该标准也被后续LLM的研究者广泛采纳，2022年基于RLHF框架提出了“宪法AI（Constitutional AI,CAI)"，即RLAIF范式，其基本思想是用强AI监督弱AI，最大程度减少人类交互的数量。  
&emsp;&emsp;由于RLHF的PPO存在不高效、不稳定（对KL超参敏感）两大问题，港科大提出了RAFT（奖励排序微调）范式，斯坦福大学提出了SuperHF范式，基本思想都是用LM生成自己的训练数据，用RM优选样本进行类SFT微调。实验证明该方法在计算上更轻量级，且比PPO稳定。更加简练的，斯坦福提出了DPO（Direct Preference Optimization）范式：核心思想是建立”奖励函数“和”最优策略“之间的映射，将偏好损失直接定义成策略的函数，从而直接用梯度下降更新策略。  
**框架步骤：**  
1、偏好数据收集  
2、训练奖励模型  
3、基于PPO（近端策略优化）的强化学习  
### 3.2.2 框架实践
（1）DeepSpeed-chat
```python
for epoch in range(args.num_train_epoch):
    for step, (batch_prompt, batch_unsupervised) in enumerate(zip(prompt_train_dataloader, unsupervised_train_dataloader)):
        # 建立经验池数据和正向传播(actor和reward）获得logits
        out = trainer.generate_experience(batch_prompt['prompt']，batch_prompt['prompt_att_mask'])
        unsup_dataset = unsup_mini_dataset.add(batch_unsupervised)

        exp_dataset = exp_mini_dataset.add(out)

        for ppo_ep in range(args.ppo_epochs):
            for i,(exp_data, unsup_data) in enumerate(zip(exp_dataset, unsup_dataset)):

                # 计算kl reward和advatage
                with torch.no_grad():
                    old_rewards = self.compute_rewards(prompts, log_probs, ref_log_probs, reward_score, action_mask)
                    advantages, returns = self.get_advantages_and_returns(old_values, old_rewards, start)

                # 计算Loss 和反向传播(actor和critic）
                actor_loss = self.actor_loss_fn(actor_log_prob[:, start:], log_probs[:, start:], advantage, action_mask[:, start])
                self.actor_model.backward(actor_loss)
                self.actor_model.step()

                critic_loss = self.critic_loss_fn(value[:, start:], old_values[:, start:], returns, action_mask[:, start:])
                self.critic_model.backward(critic_loss)
                self.critic_model.step()
```

（2）trlx

（3）trl
Transformer Reinforcement Learning
```python
for epoch in range(args.num_train_epoch):
    for query in dataloader:
        # 正向传播(actor和reward）
        response = model.generate(query)
        reward = reward_model(response)
        # 计算kl reward和advatage
        with torch.no_grad:
            rewards = self.compute_rewards(scores, active_full_logprobs, ref_full_logprobs, masks)
            values, advantages, returns = self.compute_advatage(values, rewards, masks)

        for  _ in range(self.config.ppo_epochs):
            for mini_vatch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                # 计算Loss
                loss_p, loss_v, train_stats = self.loss(old_log_probs, values, logits, vpreds, logprobs, mask, advatages, returns)
                loss = loss_p + loss_v
                # 反向传播（actor）
                self.accelerator.backward(loss)
                self.optimizer.step()
                self.optimizer.zero_grad()
```


（4）ColossalAI-Chat



