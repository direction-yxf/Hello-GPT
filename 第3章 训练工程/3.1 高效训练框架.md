### 3.1.1 高效训练框架
&emsp;&emsp;以ChatGPT为代表的生成模型对计算资源提出了巨大的需求，为了让类GPT模型训练的更快、成本更低和更高的精度，需要一个综合考虑计算和内存资源效率的大模型高效
训练框架。参考2023年5月发表的一篇综述高效训练架构的文章，我们同样以系统性视角，分享在加速内存和计算效率的最新调研进展。
**1、大模型的高效训练框架**
-内存效率。从2018年开源的GPT-1的1.17亿参数量，到GPT-2的15亿参数量，到2020年的千亿级别参数量，因此，为了发挥模型SOTA效果，往往需要海量的参数量消耗巨大的内存。提升内存效率，降低显存开销对于大模型训练至关重要。  
计算效率。ChatGPT的基础模型GPT-3训练了5万亿token，模型参数量大小达到千亿级别（175B），依据GPT-3的论文描述，它需要355个V100 GPU训练一年，训练一次成本至少460万美元。因此，提升计算效率，加快训练速度对于模型迭代和成本都非常重要。  
硬件算法协同设计。与中央处理单元（CPU）相比，图形处理单元（GPU）由于高度并行性，在执行矩阵乘法时更加强大。对于专注于特定计算任务的应用，专用集成电路ASIC具有低功耗、高训练/推理速度的优势。例如，谷歌设计的张量处理单元（TPU）比当代CPU和GPU提高了30-80倍性能。  
**内存效率**  
-Parallism，  
-Offloading,  
-Active checkpoint，  
-Quantized Training，  
-Parameter-efficient Tuning，  
**计算效率**  
-optimizer,  
-Initialization，
-Large Batch Training,  
-Incremental Learning,  
-Importance Sampling,
**硬件算法协同设计**  
-Sparse Matrix Multiplication  
-Hardware-aware Low-precision  
-Efficient Attention  

### 3.1.2 框架实践
（1）pytorch
```python
#训练设置
gradient_accumulation_steps = 8
block_size = 1024
batch_size = 8

weight_decay = 0.1
learning_rate = 2e-5
beta1 = 0.9
beta2 = 0.95

#分布式设置
backend = 'nccl'
device_type = 'cuda'

# 初始化多进程DDP
ddp = int(os.environ.get('RANK', -1)) != -1
if ddp:
    torch.distributed.init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp==0
    seed_offset = ddp_rank
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size

else:
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size

# get_batch
data_dir = ''
train_data = np.memmap(os.path.join(data_dir,'train.bin'), dtype=np.uint16, mode='r')
val_data = np.memmap(os.path.join(data_dir,'val.bin'),dtype=np.uint16, mode='r')
def get_batch(split):
    data = train_data if split =='train' else val_data
    ix = torch.randint(len(data)-block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    if device_type == 'cuda':
        x, y = x.pin_memory().to(device, non_blocking=True),y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    return x,y

# 加载模型,损失函数，学习率，优化器
model = GPT(gptconf)
model = torch.nn.parallel.DistributedDataParallel(model)
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
def get_lr(it):
    # 线性衰减
    if it<warmup_iters:
        return learning_rate * it / warmup_iters
    # 阈值衰减
    if it>lr_decay_iters:
        return min_lr

    # cosine衰减
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    assert 0<=decay_ratio<=1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate -min_lr)


# 训练
while True:
    for micro_step in range(gradient_accumulation_steps):
        if ddp:
            model.requires_grad_sync = (micro_step == gradient_accumulation_steps -1)
        with ctx:
            logits, loss = model(x, y)
            
        x, y = get_batch('train')
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        optimizer.zero_grad()
```
（2） Deepspeed

（3） Megatron
核心实现是mpu
initialize.py：负责数据并行组、张量并行组和流水线并行组的初始化，以及获取与各类并行组相关的信息
data.py：实现张量并行中的广播功能
cross_entropy.py：张量并行版本中的交叉熵；
layers.py：
mappings.py：用于张量并行的通信操作


（4） Deepspeed-megatron

（5） ColossalAI

（6）transformers



### 3.1.2 MOE框架
#### 3.1.2.1 FastMoE

#### 3.1.2.2 FasterMoE

#### 3.1.2.3 SmartMoe

