### 3.1.1 高效训练框架
以ChatGPT为代表的生成模型对计算资源提出了巨大的需求，为了让类GPT模型训练的更快、成本更低和更高的精度，需要一个综合考虑计算和内存资源效率的大模型高效
训练框架。参考2023年5月发表的一篇综述高效训练架构的文章，我们同样以系统性视角，分享在加速内存和计算效率的最新调研进展。
A Survey on Efficient Training of Transformers
内存效率
计算效率
硬件算法协同设计

### 3.1.2 框架实践
（1）pytorch
```python
#训练设置
gradient_accumulation_steps = 8
block_size = 1024
batch_size = 8

weight_decay = 0.1
learning_rate = 2e-5
beta1 = 0.9
beta2 = 0.95

#分布式设置
backend = 'nccl'
device_type = 'cuda'

# 初始化多进程DDP
ddp = int(os.environ.get('RANK', -1)) != -1
if ddp:
    torch.distributed.init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp==0
    seed_offset = ddp_rank
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size

else:
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size

# get_batch
data_dir = ''
train_data = np.memmap(os.path.join(data_dir,'train.bin'), dtype=np.uint16, mode='r')
val_data = np.memmap(os.path.join(data_dir,'val.bin'),dtype=np.uint16, mode='r')
def get_batch(split):
    data = train_data if split =='train' else val_data
    ix = torch.randint(len(data)-block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    if device_type == 'cuda':
        x, y = x.pin_memory().to(device, non_blocking=True),y.pin_memory().to(device, non_blocking=True)
    else:
        x, y = x.to(device), y.to(device)
    return x,y

# 加载模型,损失函数，学习率，优化器
model = GPT(gptconf)
model = torch.nn.parallel.DistributedDataParallel(model)
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
def get_lr(it):
    # 线性衰减
    if it<warmup_iters:
        return learning_rate * it / warmup_iters
    # 阈值衰减
    if it>lr_decay_iters:
        return min_lr

    # cosine衰减
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    assert 0<=decay_ratio<=1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate -min_lr)


# 训练
while True:
    for micro_step in range(gradient_accumulation_steps):
        if ddp:
            model.requires_grad_sync = (micro_step == gradient_accumulation_steps -1)
        with ctx:
            logits, loss = model(x, y)
            
        x, y = get_batch('train')
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        optimizer.zero_grad()
```
（2） Deepspeed

（3） Megatron
核心实现是mpu
initialize.py：负责数据并行组、张量并行组和流水线并行组的初始化，以及获取与各类并行组相关的信息
data.py：实现张量并行中的广播功能
cross_entropy.py：张量并行版本中的交叉熵；
layers.py：
mappings.py：用于张量并行的通信操作


（4） Deepspeed-megatron

（5） ColossalAI

（6）transformers



### 3.1.2 MOE框架
#### 3.1.2.1 FastMoE

#### 3.1.2.2 FasterMoE

#### 3.1.2.3 SmartMoe

