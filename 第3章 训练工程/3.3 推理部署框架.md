### 3.3.1 推理部署框架
（1）子图融合  
图融合技术即通过将多个OP（算子）合并成一个OP（算子），来减少kernel的调用。因为每一个基本OP都会对应一次GPU kernel的调用，和多次显存读写，这些都会
额外的开销。  
-FasterTransformer by NVIDIA。FT是一个用于实现基于Transformer的神经网络推理的加速引擎。FT框架是用C++/CUDA编写的，依赖于高度优化的cuBLAS、
cuBLASLt和cuSPARSELt库，与NVIDIA TensorRT等其他编译器相比，FT的特点是它支持以分布式推理Transformer大模型。图融合是FT的一个重要特征，将多层
神经网络组合成一个单一的神经网络，将使用一个单一的内核进行计算。这种技术减少了数据传输并增加了数学密度，从而加速了推理阶段的计算。例如multi-head 
attention块中的所有操作都可以合并到一个内核中。
-DeepSpeed Inference by Microsoft。对于Transformer block的计算，分为四个部分：Input Layer-Norm plus Query, Key, and Value GeMMs and their bias adds.
Transform plus Attention.Intermediate FF, Layer-Norm, Bias-add, Residual, and Gaussian Error Linear Unit (GELU).Bias-add plus Residual.
-MLC LLM by TVM。之前介绍的推理方案主要基于GPU。MLC LLM提供了可应用于移动端、消费级电脑和Web浏览器的轻设备解决方案。
（2）模型压缩
模型压缩的基本动机在于当前的模型是冗余的，可以在精度损失很小的情况下实现模型小型化，主要包括3类方法：稀疏（Sparsity）、量化（Quantization）、蒸馏（Distillation）
-稀疏。实现稀疏的一个重要方法是剪枝（Pruning），剪枝是在保证模型容量的情况下，通过修改不重要的模型权重或连接来减少模型大小。非结构化剪枝允许删除任何权重
或连接来，因此它不保留原始架构；结构化剪枝旨在维持某些元素为零的密集矩阵乘法形式。
-量化。训练后量化（Post-Training Quantization,PTQ）：模型首先经过训练以达到收敛，然后将其权重转化为较低的精度，无需进一步的训练；量化感知训练
（Quantization-Aware Training，QAT）：在预训练或进一步微调期间应用量化。
-蒸馏。知识蒸馏是一种构建更小、更便宜的模型（”student模型“）的直接方法，通过从预先训练的昂贵模型中转移技能来加速推理（"teacher模型”）融入student。  
（3）并行化
-张量并行。Tensor Parallelism，TP主要是横向增加设备数通过并行计算来减少latency。
-流水线并行。Pipeline Parallelism，PP主要是纵向增加设备数通过并行计算来支持更大模型。
（4）Transformer结构优化
-Flash Attention
-Paged Attention
-FLAT Attention
（5）动态批处理
（6）硬件升级

### 3.3.1 框架实践
（1）FasterTransformer

（2）FlashAttention

（3）vLLM

（4）FastChat

（5）GPTQ-bitsandbytes

