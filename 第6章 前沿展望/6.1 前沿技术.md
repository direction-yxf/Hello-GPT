### 6.1.1 Attention机制的改进
self-attention在Transformer中扮演着重要的角色，但在实际应用中存在两个挑战：
1、复杂性。self-attention的复杂度是序列平方乘embedding维度，因此在处理长序列时Attention模块会成为瓶颈。
2、结构先验。self-attention不假设对输入有任何结构性偏见，甚至顺序信息也需要从训练数据中学习，因此，无预训练的Transformer通常很容易在小型或中等规模的
数据上过拟合。
Attention机制的改进可以分为几个方向：
1、稀疏注意力。这一系列工作将稀疏偏差引入attention机制，从而降低了复杂性。
2、线性化注意力。这一系列工作将注意力矩阵与核特征图分解，然后以相反的顺序计算注意力以实现线性复杂度。
3、原型和内存压缩。这类方法减少了查询或键值记忆对的数量，以减少注意力矩阵的大小。
4、低秩的自注意力。这一系列工作捕获了self-attention的低秩属性
5、先验注意力。该研究领域探索用先验的注意力分布来补充或替代标准注意力。
6、改进的多头机制。这一系列工作探索了多个不同的多头（Multi-head)机制。

### 6.1.2 Transformer是终极框架吗？
RMKV
bert与GPT

### 6.1.3 超强对齐算法



### 6.1.4 大模型的鲁棒性和安全性


### 6.1.5 超长上下文
位置编码讲起：
压缩旋转嵌入
LongChat


### 6.1.6 可控生成


### 6.1.7 自优化



