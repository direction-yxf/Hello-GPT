### 5.0.1 测评综述
2023年8月，A Survey on  Evaluation of Large Language Models 综述一共调研了两百余篇文献，以评测对象（what to evaluate）、评测领域（where
to evaluate）、评测方法（how to evaluate）和目前的评估挑战等几大方面对大模型评估进行了详细的总结和梳理。我们的目标是增强对大模型当前状态的理解，
阐明他们的优势和局限性，并为其未来发展提供其意见。
1、为什么要研究大模型的测评？  
-首先，研究评测可以帮助我们更好地理解大模型的长处和短处。尽管多数研究表明大模型在诸多通用任务上已达到人类或超过人的水平，但仍然有很多研究在质疑其能力来源
是否为对训练数据的记忆。如人们发现当只给大模型输入Leetcode题目编号而不给任何编号信息的时候，大模型居然也能给正确输出答案，这显然是训练数据被污染了。
-其次，研究评测可以更好地为人与大模型的协同交互设计提供指导和帮助。大模型的服务对象终究是人，那么为了更好地进行人机交互新范式的设计，我们便有必要对其
各方面能力进行全面了解和评估。
-最后，研究评测可以更好地统筹和规划大模型未来的发展的演变、防范未知和可能的风险。
2、评估什么？
-自然语言处理：包括自然语言理解、推理、自然语言生成和多语言任务
-鲁棒性、伦理、偏见和真实性
-医学应用：包括医学问答、医学考试、医学教育和医学助手
-社会科学
-自然科学与工程：包括数学、通用科学与工程
-代理应用：使用LLMs作为代理
-其他应用
3、在哪评估？
-通用基准。big-bench helm c-eval
-领域基准。MATH
4、如何评估？
评测方法可以分为：自动评估（auto evaluation）和人工评估（human evaluation）。分类标准基于结果是否自动计算，如果可以自动计算，那就是自动评估，
否则，就是人工评估。
-自动评估。优势：速度快：迅速得到评估结果，特别是大量数据；客观性：基于预定义的指标和算法，减少偏见；可重复性：相同数据和模型得到相同的结果；成本效益：
比人工评估更经济。劣势：可能不准确：对复杂或非标准任务；缺乏深度：无法捕捉细微的质量差异；依赖标准指标：可能不适用所有任务。
常用指标：Accuracy、BLEU、ROUGE、BERTScore等
-人工指标。优势：准确性：理解和评估模型输出的复杂性；灵活性：应对非标准任务和场景；深度：提供详细和有深度的反馈。劣势：成本高：需要雇佣评估员；时间消耗：
比自动评估耗时；可能存在偏见：不同评估员的不同观点；不稳定性：文化和个体差异导致的变异性。  
5、总览  
-LLMs能够在哪些方面表现出色？
LLMs在生成文本方面展现出熟练度,能够产生流畅且准确的语言表达。
LLMs在语言理解方面表现出色，


6、重大挑战
设计AGI基准测试。什么是可靠、可信任、可计算的能正确衡量AGI任务的评估指标？




基于人工裁判的测评
-chatbot arena   superclue
基于多任务的测评
-big-bench  helm c-eval
基于模型裁判的测评
pandalm
其他单领域测评
jionlp