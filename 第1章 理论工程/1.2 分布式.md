通信基础  
分布式算法 
***
### 1.2.1 通信基础

1、Broadcast
```python
torch.distributed.broadcast()
```

2、Reduce
```python
torch.distributed.reduce()
```

3、ReduceScatter
```python
torch.distributed.reduce_scatter()
```

4、AllGather

5、AllReduce
将各个显卡的张量进行聚合（sum、min、max）后，再将结果写回各个显卡中。

6、Ring-AllReduce

7、peer2peer
```python
torch.distributed.send()
torch.distributed.recv()
```


### 1.2.2 分布式算法
```python
class DistrubutedArguments():
    nodes: int = 1
    ranks: int = 8


class Trainer():
    def __init__(self, model, train_dataset):
        self.model = model
        self.train_dataset = train_dataset

    def train(self):
        # 初始化多进程DDP
        torch.distributed.init_process_group(
            backend='nccl',
            word_size = DistrubutedArguments.nodes * DistrubutedArguments.ranks
        )
        # 分布式get_batch
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            self.train_dataset,
        )
        train_loader = torch.utils.data.DataLoader(
            self.train_dataset,
            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),
            shuffle=True,
            pin_memory=True,
            batch_size=TrainingArguments.batch_size
        )
        data_iter = iter(train_loader)
        
        # 分布式model
        model = torch.nn.parallel.DistributedDataParallel(self.model)

        # optimizer
        optimizer = torch.optim.SGD(self.model.parameters(), TrainingArguments.learning_rate)
        criterion = nn.CrossEntropyLoss()

        # train_loop
        for epoch in range(TrainingArguments.epoch):
            while True:
                batch = next(data_iter)
                x, y = batch

                # forward前向传播
                pred = model(x)
                loss = criterion(pred, y)

                # backward反向传播
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
```
模型并行和流水线并行都是对模型本身进行划分，
1、数据并行


2、模型并行

3、流水线并行

4、3D并行









