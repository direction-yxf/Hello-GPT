&emsp;&emsp;指令微调数据不同于预训练数据工程中的解析-清洗流程，由于微调数据是带有标注的数据，人工获取成本极高，因此指令微调的数据工程还包括指令数据集的构建。
### 2.2.2 数据构建
&emsp;&emsp;指令微调数据集构建方式主要分为两种：基于人工标准；基于大模型数据蒸馏：  
（1）基于人工标准    
-需要设定SFT数据标签体系，建立标签体系的大类、子类。
-构造和撰写一些prompt并且写出对应的结果。
（2） 基于大模型数据蒸馏  
-基于self-instruct生成数据，为了保证多样性，只有当一条新指令与任何现有指令的ROUGE-L重叠小于0.7时才会保留。  

&emsp;&emsp;指令微调数据集构建类别分为以下5类： 
（0）NLP任务指令构造  
-模板多样性有限，对于特定任务，输入多条任务数据，使用大语言模型（LLM）自动生成对该任务描述。给定任务描述，使用大语言模型（LLM）自动将该任务描述改写成
多样化任务模板。
-中文NLP任务指令数据匮乏
（1）单轮多任务指令  
-有种子示例的self-instruct，从种子示例prompt，利用LLM泛化大量prompt，数据类型可控、受限种子，模型脑洞不大，适用于针对性构造场景化、小批量数据。
-无种子示例的self-instruct，定义（子）话题类型，任务类型，无需给出示例，模型不受限于给定种子的限制，脑洞大，评估开放性问题指令数据质量”无示例“》”有示例“
（2）多轮指令  
ultraChat，先利用LLM生成一个Meta topic的subtopics和首轮问题，两个LLM扮演user和AI进行持续的多轮对话
多轮AIGC：风格改写，内容改写，内容修正，内容扩充，内容详细化，内容精简，需求变更等。
（3）复杂指令  
-WizardLM：将简单指令改写成复杂指令，基于规则的指令演化（深度、广度）。渐进式多步改写、逐渐拓展外延。
-规则比较死板，被随机应用到简单指令上，可能不匹配。CoT-Example：在改写prompt中添加思维链分析+示例，知道LLMs选择合理的修改指令方式，并输出符合用力风格的复杂
指令。
（4）带思考过程的指令  
先分析规划在回答
垂于场景：教科书+习题模式数据

### 2.2.3 数据质量评估
&emsp;&emsp;最近的研究发现，即使只有少量高质量的指令遵循数据，也能对大型语言模型进行高质量微调，使其表现良好。因此，如何选择高质量的数据集来对语言模型进行微调十分重要。
最近的研究工作，”Instruction Mining：High-Quality Instruction Data Selection for Large Language Models“为量化和选择高质量的指令跟随
数据提供了一个简单且可解释的方法。该工作提出了INSTRUCTMINING，一种线性质量规则和指标包，用于评估指令跟随数据的质量。  
-Length：数据集中每个回复的平均长度  
-RewardScore：数据集中每对答案的平均奖励模型推理得分  
-Perplexity：回复的指数化平均负对数可能性  
-MLTD：文本词法多样性度量  
-KNN-i：在sentenceBERT嵌入空间中近似最近邻的距离  
-Unieval-naturalness：由UniEval对话模型提供的回答是否像一个人自然会说的话的得分  
-Unieval-coherence：该回复是否作为之前对话的有效延续的得分，由UniEval对话模型提供  
-Unieval-understandability：由UniEval对话模型提供的回答是否可以理解的分数  


衡量标准-test loss，为确保推理损失能为评估数据质量提供有效的衡量标准，评估集应由经过挑选的无偏见和高质量的指令跟随样本组成。